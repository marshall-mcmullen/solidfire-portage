diff --recursive --unified --new-file a/src/sparsehash/internal/sparsehashtable.h b/src/sparsehash/internal/sparsehashtable.h
--- a/src/sparsehash/internal/sparsehashtable.h	2013-06-17 22:41:18.000000000 -0600
+++ b/src/sparsehash/internal/sparsehashtable.h	2013-06-17 22:41:19.251376650 -0600
@@ -438,6 +438,12 @@
   // Accessor function to get num_deleted.
   size_t get_num_deleted() const { return num_deleted; }
 
+  // This is used as a tag for the copy constructor, saying to destroy its
+  // arg We have two ways of destructively copying: with potentially growing
+  // the hashtable as we copy, and without.  To make sure the outside world
+  // can't do a destructive copy, we make the typename private.
+  enum MoveDontCopyT {MoveDontCopy, MoveDontGrow};
+
  private:
   // We need to copy values when we set the special marker for deleted
   // elements, but, annoyingly, we can't just use the copy assignment
@@ -449,12 +455,6 @@
     new(dst) value_type(src);
   }
 
-  // This is used as a tag for the copy constructor, saying to destroy its
-  // arg We have two ways of destructively copying: with potentially growing
-  // the hashtable as we copy, and without.  To make sure the outside world
-  // can't do a destructive copy, we make the typename private.
-  enum MoveDontCopyT {MoveDontCopy, MoveDontGrow};
-
   // DELETE HELPER FUNCTIONS
   // This lets the user describe a key that will indicate deleted
   // table entries.  This key should be an "impossible" entry --
@@ -668,7 +668,8 @@
     const size_type resize_to =
         settings.min_buckets(ht.size(), min_buckets_wanted);
     if ( resize_to > bucket_count() ) {      // we don't have enough buckets
-      table.resize(resize_to);               // sets the number of buckets
+      //table.resize(resize_to, std::min(DEFAULT_GROUP_SIZE, static_cast<uint16_t>((static_cast<uint64_t>(DEFAULT_GROUP_SIZE) * ht.size() / ht.bucket_count()))));               // sets the number of buckets
+      table.resize(resize_to);
       settings.reset_thresholds(bucket_count());
     }
 
@@ -706,7 +707,7 @@
     else                                     // MoveDontCopy
       resize_to = settings.min_buckets(ht.size(), min_buckets_wanted);
     if ( resize_to > bucket_count() ) {      // we don't have enough buckets
-      table.resize(resize_to);               // sets the number of buckets
+	  table.resize(resize_to, std::min(DEFAULT_GROUP_SIZE, static_cast<uint16_t>((static_cast<uint64_t>(DEFAULT_GROUP_SIZE) * ht.size() / ht.bucket_count()))));               // sets the number of buckets
       settings.reset_thresholds(bucket_count());
     }
 
diff --recursive --unified --new-file a/src/sparsehash/sparse_hash_map b/src/sparsehash/sparse_hash_map
--- a/src/sparsehash/sparse_hash_map	2013-06-17 22:41:18.000000000 -0600
+++ b/src/sparsehash/sparse_hash_map	2013-06-17 22:41:19.247376642 -0600
@@ -176,6 +176,16 @@
     : rep(expected_max_items_in_table, hf, eql, SelectKey(), SetKey(), alloc) {
   }
 
+  sparse_hash_map(sparse_hash_map&& source_map)
+  {
+	  ht new_rep(ht::MoveDontGrow, source_map.rep);
+	  using std::swap;
+	  swap(rep, new_rep);
+  }
+
+  sparse_hash_map(const sparse_hash_map&) = default;
+  sparse_hash_map& operator=(const sparse_hash_map&) = default;
+
   template <class InputIterator>
   sparse_hash_map(InputIterator f, InputIterator l,
                   size_type expected_max_items_in_table = 0,
@@ -192,6 +202,11 @@
   void clear()                        { rep.clear(); }
   void swap(sparse_hash_map& hs)      { rep.swap(hs.rep); }
 
+  void compact(sparse_hash_map & source_map)
+  {
+	  ht new_rep(ht::MoveDontGrow, source_map.rep);
+	  rep.swap(new_rep);
+  }
 
   // Functions concerning size
   size_type size() const              { return rep.size(); }
diff --recursive --unified --new-file a/src/sparsehash/sparsetable b/src/sparsehash/sparsetable
--- a/src/sparsehash/sparsetable	2012-02-22 13:49:42.000000000 -0700
+++ b/src/sparsehash/sparsetable	2013-06-17 22:41:19.247376642 -0600
@@ -269,6 +269,24 @@
 using GOOGLE_NAMESPACE::has_trivial_copy;
 using GOOGLE_NAMESPACE::has_trivial_destructor;
 using GOOGLE_NAMESPACE::is_same;
+
+template <typename TAlloc>
+class allocator_has_realloc {
+ private:
+  template <typename C, C>
+  struct check {
+  };
+  
+  template <typename C, typename T>
+  static char f(check<T* (C::*)(T*, size_t), &C::reallocate>*);
+  
+  template <typename C>
+  static long f(...);
+ 
+ public:
+  static const bool value = sizeof(f<TAlloc>(0)) == sizeof(char);
+};
+
 }
 
 
@@ -985,13 +1003,19 @@
 
  public:
   // Constructors -- default and copy -- and destructor
-  explicit sparsegroup(allocator_type& a) :
+  explicit sparsegroup(allocator_type& a, size_type size_of_underlying_group = 0) :
       group(0), settings(alloc_impl<value_alloc_type>(a)) {
     memset(bitmap, 0, sizeof(bitmap));
+	if (size_of_underlying_group)
+	{
+		assert(size_of_underlying_group <= GROUP_SIZE);
+		group = allocate_group(size_of_underlying_group);
+		settings.num_reserve_buckets = size_of_underlying_group;
+	}
   }
   sparsegroup(const sparsegroup& x) : group(0), settings(x.settings) {
-    if ( settings.num_buckets ) {
-      group = allocate_group(x.settings.num_buckets);
+    if ( settings.num_buckets + settings.num_reserve_buckets ) {
+      group = allocate_group(x.settings.num_buckets + settings.num_reserve_buckets);
       std::uninitialized_copy(x.group, x.group + x.settings.num_buckets, group);
     }
     memcpy(bitmap, x.bitmap, sizeof(bitmap));
@@ -1006,13 +1030,14 @@
     if ( x.settings.num_buckets == 0 ) {
       free_group();
     } else {
-      pointer p = allocate_group(x.settings.num_buckets);
+      pointer p = allocate_group(x.settings.num_buckets + x.settings.num_reserve_buckets);
       std::uninitialized_copy(x.group, x.group + x.settings.num_buckets, p);
       free_group();
       group = p;
     }
     memcpy(bitmap, x.bitmap, sizeof(bitmap));
     settings.num_buckets = x.settings.num_buckets;
+	settings.num_reserve_buckets = x.settings.num_reserve_buckets;
     return *this;
   }
 
@@ -1022,6 +1047,7 @@
     for ( int i = 0; i < sizeof(bitmap) / sizeof(*bitmap); ++i )
       std::swap(bitmap[i], x.bitmap[i]);      // swap not defined on arrays
     std::swap(settings.num_buckets, x.settings.num_buckets);
+	std::swap(settings.num_reserve_buckets, x.settings.num_reserve_buckets);
     // we purposefully don't swap the allocator, which may not be swap-able
   }
 
@@ -1030,6 +1056,7 @@
     free_group();
     memset(bitmap, 0, sizeof(bitmap));
     settings.num_buckets = 0;
+	settings.num_reserve_buckets = 0;
   }
 
   // Functions that tell you about size.  Alas, these aren't so useful
@@ -1077,14 +1104,21 @@
 
  private:
   // Create space at group[offset], assuming value_type has trivial
-  // copy constructor and destructor, and the allocator_type is
-  // the default libc_allocator_with_alloc.  (Really, we want it to have
+  // copy constructor and destructor, and the allocator_type has a
+  // reallocate function.  (Really, we want it to have
   // "trivial move", because that's what realloc and memmove both do.
   // But there's no way to capture that using type_traits, so we
   // pretend that move(x, y) is equivalent to "x.~T(); new(x) T(y);"
   // which is pretty much correct, if a bit conservative.)
   void set_aux(size_type offset, base::true_type) {
-    group = settings.realloc_or_die(group, settings.num_buckets+1);
+	  if (settings.willfit(settings.num_buckets + 1))
+	  {
+		  settings.num_reserve_buckets--;
+	  }
+	  else
+	  {
+		  group = settings.realloc_or_die(group, settings.num_buckets+1);
+	  }
     // This is equivalent to memmove(), but faster on my Intel P4,
     // at least with gcc4.1 -O2 / glibc 2.3.6.
     for (size_type i = settings.num_buckets; i > offset; --i)
@@ -1113,14 +1147,12 @@
       // Delete the old value, which we're replacing with the new one
       group[offset].~value_type();
     } else {
-      typedef base::integral_constant<bool,
-          (base::has_trivial_copy<value_type>::value &&
-           base::has_trivial_destructor<value_type>::value &&
-           base::is_same<
-               allocator_type,
-               libc_allocator_with_realloc<value_type> >::value)>
-          realloc_and_memmove_ok; // we pretend mv(x,y) == "x.~T(); new(x) T(y)"
-      set_aux(offset, realloc_and_memmove_ok());
+      //typedef base::integral_constant<bool,
+      //    (base::has_trivial_copy<value_type>::value &&
+      //     base::has_trivial_destructor<value_type>::value &&
+      //     base::allocator_has_realloc<allocator_type>::value)>
+      //    realloc_and_memmove_ok; // we pretend mv(x,y) == "x.~T(); new(x) T(y)"
+      set_aux(offset, base::true_type());
       ++settings.num_buckets;
       bmset(i);
     }
@@ -1140,8 +1172,8 @@
 
  private:
   // Shrink the array, assuming value_type has trivial copy
-  // constructor and destructor, and the allocator_type is the default
-  // libc_allocator_with_alloc.  (Really, we want it to have "trivial
+  // constructor and destructor, and the allocator_type has a
+  // reallocate function.  (Really, we want it to have "trivial
   // move", because that's what realloc and memmove both do.  But
   // there's no way to capture that using type_traits, so we pretend
   // that move(x, y) is equivalent to ""x.~T(); new(x) T(y);"
@@ -1156,6 +1188,7 @@
     for (size_type i = offset; i < settings.num_buckets-1; ++i)
       memcpy(group + i, group + i+1, sizeof(*group));  // hopefully inlined!
     group = settings.realloc_or_die(group, settings.num_buckets-1);
+	settings.num_reserve_buckets = 0;
   }
 
   // Shrink the array, without any special assumptions about value_type and
@@ -1168,6 +1201,7 @@
                             p + offset);
     free_group();
     group = p;
+	settings.num_reserve_buckets = 0;
   }
 
  public:
@@ -1182,14 +1216,12 @@
         free_group();
         group = NULL;
       } else {
-        typedef base::integral_constant<bool,
-            (base::has_trivial_copy<value_type>::value &&
-             base::has_trivial_destructor<value_type>::value &&
-             base::is_same<
-                 allocator_type,
-                 libc_allocator_with_realloc<value_type> >::value)>
-            realloc_and_memmove_ok; // pretend mv(x,y) == "x.~T(); new(x) T(y)"
-        erase_aux(offset, realloc_and_memmove_ok());
+        //typedef base::integral_constant<bool,
+        //    (base::has_trivial_copy<value_type>::value &&
+        //     base::has_trivial_destructor<value_type>::value &&
+        //     base::allocator_has_realloc<allocator_type>::value)>
+        //    realloc_and_memmove_ok; // pretend mv(x,y) == "x.~T(); new(x) T(y)"
+        erase_aux(offset, base::true_type());
       }
       --settings.num_buckets;
       bmclear(i);
@@ -1234,7 +1266,7 @@
       return false;
     // We'll allocate the space, but we won't fill it: it will be
     // left as uninitialized raw memory.
-    group = allocate_group(settings.num_buckets);
+    group = allocate_group(settings.num_buckets + settings.num_reserve_buckets);
     return true;
   }
 
@@ -1281,36 +1313,37 @@
   bool operator>=(const sparsegroup& x) const { return !(*this < x); }
 
  private:
-  template <class A>
-  class alloc_impl : public A {
-   public:
-    typedef typename A::pointer pointer;
-    typedef typename A::size_type size_type;
-
-    // Convert a normal allocator to one that has realloc_or_die()
-    alloc_impl(const A& a) : A(a) { }
-
-    // realloc_or_die should only be used when using the default
-    // allocator (libc_allocator_with_realloc).
-    pointer realloc_or_die(pointer /*ptr*/, size_type /*n*/) {
-      fprintf(stderr, "realloc_or_die is only supported for "
-                      "libc_allocator_with_realloc\n");
-      exit(1);
-      return NULL;
-    }
-  };
+//  template <class A, bool KHasRealloc = base::allocator_has_realloc<A>::value>
+//  class alloc_impl : public A {
+//   public:
+//    typedef typename A::pointer pointer;
+//    typedef typename A::size_type size_type;
+//
+//    // Convert a normal allocator to one that has realloc_or_die()
+//    alloc_impl(const A& a) : A(a) { }
+//
+//    // realloc_or_die should only be used when using an allocator with a
+//    // reallocate function
+//    pointer realloc_or_die(pointer /*ptr*/, size_type /*n*/) {
+//      fprintf(stderr, "realloc_or_die is only supported for allocators "
+//                      "with function Alloc::reallocate(pointer p, size_type n) "
+//                      "(such as libc_allocator_with_realloc)\n");
+//      exit(1);
+//      return NULL;
+//    }
+//  };
 
   // A template specialization of alloc_impl for
   // libc_allocator_with_realloc that can handle realloc_or_die.
   template <class A>
-  class alloc_impl<libc_allocator_with_realloc<A> >
-      : public libc_allocator_with_realloc<A> {
+  class alloc_impl
+      : public A {
    public:
-    typedef typename libc_allocator_with_realloc<A>::pointer pointer;
-    typedef typename libc_allocator_with_realloc<A>::size_type size_type;
+    typedef typename A::pointer pointer;
+    typedef typename A::size_type size_type;
 
-    alloc_impl(const libc_allocator_with_realloc<A>& a)
-        : libc_allocator_with_realloc<A>(a) { }
+    alloc_impl(const A& a)
+        : A(a) { }
 
     pointer realloc_or_die(pointer ptr, size_type n) {
       pointer retval = this->reallocate(ptr, n);
@@ -1330,11 +1363,13 @@
   class Settings : public alloc_impl<value_alloc_type> {
    public:
     Settings(const alloc_impl<value_alloc_type>& a, u_int16_t n = 0)
-        : alloc_impl<value_alloc_type>(a), num_buckets(n) { }
+        : alloc_impl<value_alloc_type>(a), num_buckets(n), num_reserve_buckets(0) { }
     Settings(const Settings& s)
-        : alloc_impl<value_alloc_type>(s), num_buckets(s.num_buckets) { }
+        : alloc_impl<value_alloc_type>(s), num_buckets(s.num_buckets), num_reserve_buckets(s.num_reserve_buckets) { }
 
     u_int16_t num_buckets;                    // limits GROUP_SIZE to 64K
+	u_int16_t num_reserve_buckets;            // limits GROUP_SIZE to 64K
+	bool willfit(size_type n) const { return n <= num_buckets + num_reserve_buckets; }
   };
 
   // The actual data
@@ -1509,8 +1544,8 @@
   size_type num_nonempty() const   { return settings.num_buckets; }
 
   // OK, we'll let you resize one of these puppies
-  void resize(size_type new_size) {
-    groups.resize(num_groups(new_size), group_type(settings));
+  void resize(size_type new_size, size_type underlying_entries_hint = 0) {
+    groups.resize(num_groups(new_size), group_type(settings, underlying_entries_hint));
     if ( new_size < settings.table_size) {
       // lower num_buckets, clear last group
       if ( pos_in_group(new_size) > 0 )     // need to clear inside last group
